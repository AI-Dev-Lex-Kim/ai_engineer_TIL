# BERT의 [CLS] 토큰 이해하기

BERT 모델에 입력값으로 문장이 들어갔을때, 토큰화를 거치게 된다.

자연어를 모델이 이해할 수 있는 숫자로 바꿔주어야한다.

이때 토큰화를 거쳐서 각 토큰을 사전(Voca)으로 만들어 토큰에 해당하는 인덱스 정수로 변환 해준다.

<br>

이 과정에서 토큰으로 변하게 된다면, 문장의 시작이 어디있는지 끝이 어디 인지를 알아야한다.

`[CLS]`라는 특별한 토큰을 문장 가장 앞에 넣어주어, 문장의 시작이라고 알려주는것이다.

(`[CLS]`라고 정한것은 BERT를 개발할때, 정의한것이다.)

<br>

[CLS]라는 토큰은 마찬가지로 모델이 이해할 수 있는 숫자로 만들기위해 Voca에 들어가 Voca의 [CLS] 인덱스인 정수로 변환한다.

<br>

Bert는 특이하게도 BERT를 만든 사람들이 [CLS] 토큰이 단지 문장의 시작의 의미로만 사용하지 않았다.

[CLS] 토큰 자체가 문장의 모든 문맥을 함축하고 있다고 학습하게 시켰다.

문장 내의 모든 단어와의 관계를 구한것이 [CLS] 토큰에 들어간것이다.

<br>

[CLS]에 담긴 정보를 활용해서 다양한 Downstream task로 활용할 수 있게 만들었다.

[CLS] 토큰을 사용해서 ‘분류’, ‘유사도 비교’, ‘회귀’ 등등 다양한 태스크를 처리할 수 있게 되는것이다.

<br>

BERT라는 기본 모델(AutoModel)은 마지막 레이어에서 2가지를 반환한다.

‘**last_hidden_state**’와 ‘**pooler_output**’이다.

<br>

**lasst_hidden_state**는 최종 은닉 상태이다.

BERT는 12개의 hidden layer를 가지고 있다.

그 레이어를 모두 거치면서 계산된 768차원의 계산 결과가 담겨져 있다.

<br>

**pooler_output**

BERT 모델에 입력 문장을 넣었을때, [CLS] 토큰을 가장 앞에 넣어주었다.

[CLS] 토큰에 대한 값만을 뽑아준것이다.

shape는 [1, 768]이 된다.

입력 문장을 768차원으로 확장시켜서 학습시켜서 가장 768차원이 된것이다.

<br>

정리하지면 last_hidden_state는 입력 문장을 12개의 layer에 거쳐 계산된 결과이고

이 결과에서 [CLS] 토큰을 뽑아 nn.Linear를 한번 거쳐서 나온 결과가 pooler_output이다.

<br>

pooler_output을 가지고 다양한 task를 처리하면 된다.

<br>

5가지 카테고리로 분류하는 문제를 하고싶다면, pooler_output이 (1, 768)에서 (1, 5)로 만들어 주면 되는것이다.

nn.Linear(768, 5)를 거치면 5개의 차원이 만들어지게 되고

이어서 Softmax 함수를 사용해서 logit을 반환하면 된다.

<br>

이런식으로 [CLS] 토큰을 활용해서 Head만 바꾸면 다양한 태스크를 공통적으로 처리할 수 있다는 엄청난 편리한 워크플로우를 가지고 있다.

<br>

참고

- https://chanmuzi.tistory.com/243
