- [LoRA 등장 배경](#lora-등장-배경)
  - [1. 저장 공간](#1-저장-공간)
  - [2. 배포](#2-배포)
  - [3. 학습](#3-학습)
  - [4. 치명적 망각(catastrophic forgetting)](#4-치명적-망각catastrophic-forgetting)
- [LoRA 개념](#lora-개념)
  - [초기화 전략](#초기화-전략)
  - [스케일링 계수(scaling factor)](#스케일링-계수scaling-factor)
  - [왜 이렇게 적은 연산으로도 성능이 나올 수 있을까?](#왜-이렇게-적은-연산으로도-성능이-나올-수-있을까)
  - [다운 프로젝션 → 업 프로젝션 하는 이유는?](#다운-프로젝션--업-프로젝션-하는-이유는)
  - [d가 뭐야?](#d가-뭐야)
  - [r, Rank](#r-rank)
- [추론시 가중치 병합(Weight Merging)](#추론시-가중치-병합weight-merging)
- [Transformer LoRA 적용](#transformer-lora-적용)
- [Pytroch 코드](#pytroch-코드)
- [정리](#정리)

---

## LoRA 등장 배경

대형 언어 모델(GPT-3, BERT 등)은 수십억 개 이상의 파라미터를 가지고 있다.

Full Fine-Tuning, FFT는 사전 훈련 모델의 모든 파라미터를 새로운 데이터셋에 맞추어 업데이트를 시킨다.

모델의 사이즈가 커짐에 따라 심각한 문제가 생겼다.

<br>

### 1. 저장 공간

예를 들면

풀 파인튜닝 방식에서 여러 task 마다 각각 복사해 저장한다.

- 감정 분석용 모델: W_task1(6GB)
- 요약용 모델: W_task2 (6GB)
- 질의응답용 모델: W_task3 (6GB)

→ 3개 task만 해도 18GB 저장 공간이 필요하다.

<br>

LoRA 방식은 딱 한번만 사전 학습 모델을 가져오면 된다.

- 공통 W₀: 6GB (단 1번 저장)
- 감정 분석용 LoRA adapter: ΔW_task1(10MB)
- 요약용 LoRA adapter: ΔW_task2(10MB)
- 질의응답용 LoRA adapter: ΔW_task3(10MB)

→ 총 저장량은 6GB + 30MB = 약 6.03GB이다.

<br>

### 2. 배포

두번째 장점으로 배포 측면도 있다.

LoRA는 서버에 Wo 하나만 불러온다.

각 요청 task에 따라 필요한 LoRA adapter만 불러와서 조립한다.

```python
model.load_adapter("lora-emotion.pt")  # 감정 분석
model.load_adapter("lora-summary.pt")  # 요약
model.load_adapter("lora-qa.pt")       # 질의응답
```

<br>

### 3. 학습

세번째 장점으로 학습 측면도 있다.

Full Fine-Tuning은 W을 전체 학습해야한다.

LoRa는 B, A 두 행렬만 학습해서 메모리를 적게 쓰고 빠르다.

<br>

### 4. 치명적 망각(catastrophic forgetting)

모델의 모든 가중치를 새로운 작업에 맞추어 변경하면,

사전학습 단계의 방대한 지식을 잃어버릴수있다.

<br>

## LoRA 개념

> 기존 모델의 가중치 W는 그대로 두고(freeze),
> 추가로 아주 작은(low rank) 두 개의 행렬(B, A)을 만들어서
> task에 알맞은 W의 변화량만 학습하자.

<br>

LoRA는 궁극적으로 원하는 가중치 W를 만들기 위해서,

기존의 사전 학습된 가중치 $W_o$에 두 개의 학습 가능한 저차원 임베딩 벡터 B, A를 행렬곱한 결과를 더하는 방식이다.

```python
W = W₀ + ΔW
ΔW = B @ A
```

A → 다운 프로젝션(고차원 → 저차원)

B → 업 프로젝션(저차원 → 고차원)

<br>

이때 두 행렬(B, A)는 학습을 통해 점점 업데이트 되면,

최종적으로 `ΔW = B @ A` 형태로 표현되며 `W = W₀ + ΔW` 가 되어 모델이 task에 맞게 적응 된다.

ΔW는 downstream task에 알맞은 원하는 가중치 행렬이 되기 위해 $W_o$에 더해주는 행렬

점점 ΔW이 되어가는것이라고 생각하면됨. 학습중에는 ΔW이 아직 아님.

<br>

정리하자면, 사전 학습된 가중치 행렬을 하나도 안건드림.

Full Fine Tuning이 아님. `ΔW = B @ A` 만 학습하면서 `W` 전체를 바꾸는것 처럼 효과를 낸다.

여기서 B, A는 저차원 행렬이라 계산량이 작음.

<br>

연산량을 예시로 비교해보자.

입력 x가 d차원일때

Full Fine-Tuning은 `$W_o$ @ x = 4096 × 4096 = 1,600만` 이다.

<br>

LoRA 연산은

- A @ x → `r × d = 8 × 4096 = 32,768`
- B @ (...) → `d × r = 4096 × 8 = 32,768`
- 합계는 약 6.5만 정도 된다.

즉 원래 연산 대비 0.4% 수준 밖에 안된다.

<br>

### 초기화 전략

안정적인 학습을 위해 LoRA는 독특한 초기화 전략을 사용한다.

훈련 시작 시점에서 모델에 아무런 영향을 미치지 않도록 만든다.

이를 위해서 아래와 같이 만든다.

- A는 랜덤 가우시안으로 초기화 한다.
- B는 0으로 초기화되어 학습을 시작한다.
- ΔW = B @ A는 학습 시작시 0이 되도록 한다.

<br>

결과적으로 훈련 초기에는 BA가 0행렬이 되어 모델은 사전훈련된 상태에서 학습을 시작하게 된다.

<br>

### 스케일링 계수(scaling factor)

LoRA는 학습된 업데이트의 크기를 조절하기 위해서 scaling factor를 도입했다.

$$
h = W_0x + \frac{\alpha}{r}BA_x
$$

여기서 $\alpha$는 lora_alpha로 하이퍼파라미터이다. 이 스케일링 계수는 rank r의 변화에 따라 하이퍼파라미터 재조정의 필요성을 줄여주며, 학습된 적응의 강도를 조절하는 역활을 한다.

<br>

즉, $\alpha$는 learning rate와 비슷하게 생각하면 된다.

r이 높은 값이 된다면, $\alpha$가 영향을 받아 훈련이 불안정 해지는 문제가 생긴다.

이를 해결하기 위해서 rsLoRA(Rank-Stablized LoRA)가 나왔다.

r이 증가할때 스케일링 계수가 급격히 감소하는것을 방지해, 높은 rank에서도 안정적으로 학습을 가능하게 한다.

hugging face PEFT 라이브러리에서 `use_rsloar=True` 옵션을 통해 활성화 할 수 있다.

<br>

### 왜 이렇게 적은 연산으로도 성능이 나올 수 있을까?

대부분 파라미터($W_o$)는 이미 사전 학습된 가중치이므로 잘 학습되어 있다.

특정 task에 필요한 “미세 조정”만 해주면 된다.

그래서 저랭크 행렬을 태스크에 알맞게 학습만 시켜줘도 충분한 성능이 나온다.

즉, d x d 사이즈의 가중치 행렬을 가진 ΔW이 아니어도,

작은 r x d, d x r 행렬로도 충분히 근사할 수 있다는 것이 실험적으로 증명된 것이다.

<br>

### 다운 프로젝션 → 업 프로젝션 하는 이유는?

표현력을 유지하면서 구조를 단순화 하기 위해서이다.

단순히 ΔW를 작게 만들면 표현력이 부족해질 수 있다.

하지만 ΔW를 저랭크(low-rank) 구조로 만들면, 정보 압축 + 의미 있는 학습이 가능하다.

저차원 표현을 다시 원래 차원으로 확장해서 기존 모델과 결합도 가능하다.

| 단계          | 목적                                |
| ------------- | ----------------------------------- |
| 다운 프로젝션 | 차원을 줄여 파라미터/계산량 절감    |
| 업 프로젝션   | 다시 원래 차원으로 복원해 ΔW로 사용 |

<br>

### d가 뭐야?

d는 기존 모델에서 사용하던 원래 차원, 즉 embedding size or hidden size를 의미한다.

예를들어 GPT-3에서의 hidden size = 4096이다. 그러면 d = 4096이 된다.

원래 모델의 가중치는 d x d 행렬이다.

<br>

### r, Rank

r은 “low-rank”차원으로 LoRa에서 새로 추가한 임베딩 크기이다.

얼마나 **“압축된 정보로 W를 표현할지”** 조절하는 크기이다.

<br>

r이 클수록 어댑터는 더 복잡하고 미세한 패턴을 학습할 수 있어 표현력이 좋아진다.

하지만 동시에 학습할 파라미터 수가 늘어나 메모리 사용량과 계산 비용이 증가하고 과적합 위험도 커진다.

<br>

LoRA에서 가장 놀라운 점은 매우 낮은 r 값으로도 FFT와 정말 비슷한 높은 성능을 달성할 수 있다는 점이다.

$W_q$와 $W_v$에 r=1만으로도 73.4%의 정확도를 달성했다. r=64일때 73.5%와 거의 차이가 없다.

즉, 성능이 r 값에 따라 무한정 증가하는것이 아니라, 특정 지점에서 수렴하거나 약간 감소하는 경향을 보인다는 것이다.

또한 r=8, r=64로 학습된 어댑터의 벡터들의 방향이 비슷한것으로 발견됐다.

중요한 정보가 매우 낮은 차원의 부분 공간에 집중되어있다.

r 값을 늘리면 추가되는 차원들은 덜 중요한 정보나 학습 과정에서의 축적된 노이즈를 포함할 수 있다.

따라서 실제로 작업을 할때는 4, 8, 16, 32 같이 상대적으로 낮은 r 값 부터 시작하는게 좋다.

<br>

예시를 들어보자.

GPT-3급 차원 d = 4096이다.

r = 8이다.

- A: `8 × 4096` → 32,768개 파라미터
- B: `4096 × 8` → 32,768개 파라미터

Full Fine-Tuning 했을때 더해지는 가중치 행렬인 ΔW는 `4096 × 4096` 차원 이였다.

LoRA는 r=8 짜리 행렬 3개로 ΔW에 최대한 근사치하게 표현한다.

<br>

정리하자면, r은 LoRA에서 ΔW를 표현하기 위해 저차원(hidden) 공간의 크기이며,

W 변화량을 얼마나 정교하게 표현할 것인지 결정하는 하이퍼파라미터 이다.

<br>

## 추론시 가중치 병합(Weight Merging)

훈련이 완료된 후, 학습된 저 rank 행렬 A, B의 곱인 BA를 원래의 사전학습 가중치 행렬 $W_0$에 직접 더한다.

새로운 가중치 행렬 $W’ = W_0 + BA$을 얻는다. 이렇게 병합된 모델 W’는 원래 사전학습 가중치 모델과 동일한 아키텍처와 파라미터 수를 가진다.

추론 시 어떠한 추가적인 계산 오버헤드나 지연시간도 발생하지 않는다.

다른 Adpater 기반 방법들은 모델에 새로운 레이어를 추가하기 때문에 추론시 지연시간이 증가한다.

<br>

## Transformer LoRA 적용

LoRA 논문에서는 단순성과 매개변수 효율성을 극대화하기 위해서 주로 self-attention 가중치 행렬에만 LoRA를 적용했다.

MLP 모듈은 frozen하는 전략을 채택했다. attention 메커니즘은 입력 시퀀스 내의 단어 간 관계를 학습하는 핵심적인 부분이다.

따라서 attention 부분만 새로운 task에 맞게 조정하는것이 효과적일것이라고 했다.

<br>

다양한 조합을 테스트한 결과 Query($W_q$)와 Value($W_v$) 행렬에 동시에 LoRA를 적용하는 것이 가장 일관되게 좋은 성능을 보여줬다.

하지만, 각각 하나에만 적용하면 성능이 안좋아졌다. 이러한 점은 하나의 행렬에 높은 rank를 할당하는것보다 여러 행렬에 걸쳐 low-rank을 적용하는게 효과적인것으로 증명했다.

<br>

## Pytroch 코드

```python
import torch
import torch.nn as nn

d = 4096
r = 8

class LoRALayer(nn.Module):
    def __init__(self, d, r):
        super().__init__()
        self.A = nn.Linear(d, r, bias=False)  # 다운 프로젝션
        self.B = nn.Linear(r, d, bias=False)  # 업 프로젝션

    def forward(self, x):
        return self.B(self.A(x))  # B @ (A @ x)

# 입력
x = torch.randn(1, d)

# 기존 W₀는 고정된 채로 사용한다고 가정
W0 = torch.randn(d, d)
baseline = x @ W0.T

# LoRA output
lora_layer = LoRALayer(d, r)
delta = lora_layer(x)
output = baseline + delta  # W₀ x + ΔW x
```

<br>

## 정리

B, A의 저차원 행렬로 인해 계산량과 파라미터 수가 매우 작다.

하지만 학습 성능은 거의 Full Fine-Tuning에 가깝다.

LoRA가 가볍지만 효과가 좋은 이유가 이런 구조 이기 때문이다.

<br>

참고

- [LoRA Hugging Face Docs](https://huggingface.co/docs/peft/task_guides/lora_based_methods)
- [**LoRA: Low-Rank Adaptation of Large Language Models 논문**](https://arxiv.org/pdf/2106.09685)
