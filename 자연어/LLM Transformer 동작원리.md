- [LLM Transformer 동작원리](#llm-transformer-동작원리)
  - [임베딩](#임베딩)
    - [단어 벡터](#단어-벡터)
    - [내적](#내적)
    - [벡터 차원](#벡터-차원)
    - [입력 값 임베딩](#입력-값-임베딩)
  - [Un-Embedding](#un-embedding)
    - [Un-Embedding 행렬 연산](#un-embedding-행렬-연산)
    - [소프트 맥스 연산](#소프트-맥스-연산)

# LLM Transformer 동작원리

<br>

## 임베딩

입력값은 <mark>**컴퓨터가 이해 가능한 언어로 바꾸어 주어야한다.**</mark>

그 과정을 임베딩 단계에서 실행한다.

<br>

### 단어 벡터

그전에 먼저 <mark>**임베딩 행렬이 무엇**</mark>인지 알아야한다.

임베딩 행렬은 <mark>**수많은 단어들을 사전학습을 통해 임베딩 행렬에 저장**</mark>해둔다.

어떤것을 저장하냐면, 단어들마다 의미가 있다. 사전학습으로 저장하는 <mark>**단어들과 단어들의 의미를 숫자형식으로 저장**</mark>한다.

이 숫자는 단순한 정수형태가 아니라 실수형태로 벡터 형식으로 저장을 한다.

벡터라는 것은 다음과 같은 형태이다.

```python
hi = [0.58, 2.19]
hello = [3.14, 2.86]
```

두단어는 두개 실수 요소로 저장되어 있다.

이 값은 <mark>**벡터 공간이 위치 좌표에 존재한것과 같은 개념**</mark>이다.

hi는 (x, y)라고 할때, (0.58, 2.19) 라는 좌표평면에 존재하는 것이고

hello는 (3.14, 2.86) 이라는 좌표평면에 존재하는 것이다.

<br>

### 내적

이 두 단어는 좌표를 바탕으로 <mark>**거리를 계산해서 얼마나 단어가 유사한지 유사도를 계산**</mark>할 수 있다.

두 단어의 유사도를 계산할때는 <mark>**“내적” 이라는 개념을 사용**</mark>한다.

<mark>**두 단어의 각각의 요소를 곱해서 더해주는 것**</mark>이다.

$$
hi = [v_1, v_2]
\\
hello = [w_1, w_2]
\\
내적 = v_1 * w_1 + v_2 + w_2
$$

내적의 값은 0.58 _ 3.14 + 2.19 _ 2.86 = 8.0846 이라는 값이 나왔다.

이 두 단어의 유사도는 8.0846 이다.

<br>

이렇게 단어들 끼리의 유사도를 수치값으로 계산할 수 있다.

<mark>**유사할수록 비슷한 위치의 좌표에 존재**</mark>해 있다.

<mark>**좌표의 방향도 비슷**</mark>하게 되어있다.

<br>

방향이라는것은 좌표 평면상에서 (0, 0)을 기준으로 해당 단어에 직선으로 그었을때 화살표 방향을 말하는 것이다.

예를 들어서 hi는 (0, 0)에서 (0.58, 2.19) 방향으로 선을 직선으로 그어주면 직선의 방향이 나올것이다.

두 단어가 비슷하지 않다면, 좌표와 방향도 다르게 위치할 것이다.

<br>

### 벡터 차원

지금까지 2차원 좌표평면 상으로 두 단어를 표현했다.

2차원이라는것은 방금 처럼 (x, y) 좌표로 만든것이다. hi는 (0.58, 2.19) 이렇게 2차원 좌표에 있다.

만약 <mark>**3차원 좌표로 만든다면, 더 풍부하게 단어들 끼리의 의미를 잘 표현할 수 있을것**</mark>이다.

3차원으로 만든다면 hi는 다음과 같은 벡터가 될것이다.

```python
hi = [0.123, 0.842, 1.290]
```

이런식으로 요소가 한개더 추가되었다.

실제 임베딩 모델로 단어를 임베딩 할때는 <mark>**2차원, 3차원으로 표현하지 않고 더 많은 차원으로 표현**</mark>한다.

768차원, 1024차원 등등 같이 엄청나게 큰 차원으로 단어를 벡터화 한다.

<br>

임베딩은 사전학습시 수많은 단어를 학습한다. 영어같은 경우는 보통 5만개 이상의 단어를 학습한다고 한다.

이 5만개의 단어들의 관계를 바탕으로 단어를 벡터로 수치화 한것이다.

<mark>**즉, 임베딩이라는 것은 단어들과 단어들의 관계를 벡터로 수치화 한것이다.**</mark>

이렇게 5만개 이상의 <mark>**단어들이 모여있는 것을 임베딩 행렬**</mark>이라고 한다.

<br>

### 입력 값 임베딩

다시 돌아가서 입력값을 임베딩 하는것에 대해 이야기해보자.

예를 들어 다음과 같은 입력값이 주어졌다.

```python
input = 내가 가장 좋아하는 가수는 사랑했나봐 라는 노래를 불렀던
```

이제 입력값이 들어갔으니 그 다음 단어를 예측해야한다.

그러기 위해서는 컴퓨터가 이해할 수 있게 임베딩을 해야한다.

<br>

임베딩을 하기전에 가장 먼저 토큰화를 진행 해야한다.

```python
tokens = [‘내가’, ‘가장’, ‘좋아하는’ ,‘가수는’ ,‘사랑했나봐’ ,‘라는’, ‘노래를’, ‘불렀던’]
```

이렇게 토큰이 만들어졌다면, 이 토큰들을 임베딩 해주어여한다.

<br>

사전 학습을 통해 수많은 단어를 가지고 있던 임베딩 행렬에는 이미 저 단어들이 들어가 있다.

그 단어들을 임베딩 행렬에서 찾아서 가져온다.

예를 들어서 다음과 같이 나올것이다.

```python
내가 = [0.18, 0.1282, ...]
가장 = [2.1934, 9.2845, ...]
...
부른 = [4.929, 7.9912, ...]
```

이 단어들은 임베딩 벡터 공간에서 각각 단어의 유사성을 바탕으로 만들어진 벡터 값인 것이다.

하지만, 각 문장의 문맥을 바탕으로 다음 단어를 예측해야한다.

<mark>**문맥을 파악하려면 단어별로 문장의 어디에 위치되어 있는지에 대한 위치 정보도 필요한 것**</mark>이다.

그 위치 정보를 알아야, 문맥을 파악하는 것이다.

참고로 문맥은 Context라고 영어로 표현한다.

<br>

문장이 꽤 길다면, 그 안의 단어(토큰)들도 굉장히 많을 것이다.

<mark>**즉, 단어의 크기에 따라 모델의 계산량도 영향을 끼친다는 것이다.**</mark>

<br>

그래서 <mark>**LLM을 사용할때, 문장의 길이를 제한**</mark>을 둔다.

이전에 말했던것 처럼 문맥은 영어로 Context이다.

LLM을 사용할때 흔히 <mark>**Context Size를 제한을 두는 것이다.**</mark>

<br>

이후 수많은 레이어들이 가지고 있는 가중치를 바탕으로 각 단어의 벡터값과 행렬곱을 진행한다.

각 단어 벡터들 끼리의 연산은 이루어 지지 않고 <mark>**레이어에 있는 가중치 행렬과 각각 단어의 행렬연산이 이루어진다.**</mark>

레이어에서 각각 단어들과 가중치 행렬의 연산이 단독으로 이루어진다.

따라서 병렬 방식으로 연산이 가능한 것이다.

<br>

## Un-Embedding

이 과정을 거친뒤 마지막으로 Un-Embedding 레이어를 거친다.

임베딩 레이어와 반대되는 개념이라고 할 수 있다.

임베딩 레이어는 토큰화된 단어를 숫자값인 벡터로 변환 시켰다.

<mark>**언-임베딩 레이어에서는 단어 벡터를 확률 분포로 변환한다.**</mark>

<mark>**더 쉽게 말해서 이제 가장 마지막 레이어인 언-임베딩 레이어에서 다음 단어를 예측한다는 것이다.**</mark>

<br>

이 과정은 2가지 단계로 나눠서 진행된다.

1. Un-Embedding 행렬 연산
2. 소프트 맥스 함수 연산

<br>

### Un-Embedding 행렬 연산

첫번째인 Un-Embedding 행렬 연산을 알아보자.

<mark>**언-임베딩 행렬과 문장의 마지막 단어 벡터와의 연산을 하는것이다.**</mark>

<br>

언-임베딩 행렬은 임베딩 행렬과 비슷하다. $W_u$ 행렬이라고도 부른다.

임베딩 행렬과 마찬가지로 사전학습 당시 학습한 모든 단어의 행렬이다.

예를들어 5만개의 단어가 있다고 해보자.

```python
input = 내가 가장 좋아하는 가수는 사랑했나봐 라는 노래를 불렀던
```

마지막 단어는 ‘불렀던’ 이라는 단어 이다.

<br>

<mark>**이제 5만개의 단어 벡터를 가진 언-임베딩 행렬과 ‘불렀던’이라는 단어의 벡터와 행렬곱 연산을 해준다.**</mark>

그결과로 5만개의 값을 가진 벡터가 나온다.

(50000, 1) 형태로 나온다. 이 값은 크고 작은 값들이 될것이다.

예를들어

```python
last_vector = [+256.7, -123.2, ..., 0.123]

len(last_vector)
-> 5만개
```

<mark>**5만개의 각각의 단어별로 값을 가지게 된것과 다름없다.**</mark>

<br>

### 소프트 맥스 연산

이제는 두번째 단계였던 소프트 맥스 함수 연산을 해주어야한다.

<mark>**이전에 확률 분포로 변환하여 다음 단어를 예측한다고 했다.**</mark>

<br>

확률 분포로 변환한다는 것은 무엇일까?

<mark>**다음 단어를 예측할때, 확률적으로 어떤 다음 단어가 오는게 좋을지 5만개의 각 단어별로 확률을 구하는것이다.**</mark>

<br>

이때 사용하는 것이 소프트맥스 함수이다.

그래서 대부분 소프트맥스 레이어라고도 부른다.

<mark>**소프트 맥스 함수를 사용하면, 5만개의 단어를 0과 1사이의 값으로 확률분포로 변환한다.**</mark>

그렇다면 확률 분표 값으로 만든 모든 단어들의 값을 합치면 1이 되어야한다.

<br>

<mark>**이후 가장 높은 값인 단어가 그 다음 단어가 되는것이다.**</mark>

<br>

여기서 Temperture이라는 파라미터를 LLM에서 주기도 한다.

<mark>**Temperture는 소프트 맥스 방정식에서 지수의 분모에 값을 추가하는 것이다.**</mark>

<mark>**지수의 분모에 추가하다보니, Temperture의 값이 커질수록 점점 고르게 확률이 분포하게 된다.**</mark>

따라서 다음 예측값의 다양성을 원한다면 Temperture을 줄 수 있는것이다.

값이 크다면, 문장이 이상한 값으로 나오게 될것이다. 그래서 실제로 chat-gpt는 2이상의 값을 못주게 한다.

<br>

참고

- [https://www.youtube.com/watch?v=g38aoGttLhI&ab_channel=3Blue1Brown한국어](https://www.youtube.com/watch?v=g38aoGttLhI&ab_channel=3Blue1Brown%ED%95%9C%EA%B5%AD%EC%96%B4)
- [https://en.wikipedia.org/wiki/Transformer\_(deep_learning_architecture)#Positional_encoding](<https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#Positional_encoding>)
