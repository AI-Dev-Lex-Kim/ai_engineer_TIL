# Precision, Recall, F1-Score

![image.png](../images/Precision,%20Recall,%20F1-Score%20/image.png)

True → 모델이 예측한게 맞았음

False → 모델이 예측한게 틀림

<br>

Positive → 모델이 양성으로 예측

Negative → 모델이 음성으로 예측

<br>

True Positive(TP) → 모델이 양성으로 예측해 정답

True Negative(TN) → 모델이 음성으로 예측해 정답

False Positive(FP) → 모델이 양성으로 예측해 틀림

False Negative(FN) → 모델이 음성으로 예측해 틀림

<br>

## Precision(정밀도)

모델이 Positive(양성)이라고 예측한 것 중에서 실제로 True(정답)인 비율.

$$
Precision = \frac{TP}{TP + FP}
$$

모델이 Positive라고 예측한 것 중에 → TP + FP

실제로 True(정답)인 비율 → TP

<br>

FP → 모델이 실수로 양성이라고 판단한것

<br>

### Precision 평가 의미

모델이 Positive라고 예측한 결과들에 대해 얼마나 믿을 수 있는지(신뢰도) 확인하기 위해서 사용한다.

<br>

### 값

<mark>**1.0**</mark>

모델이 양성으로 판단한 모든 케이스에서 다 양성이다.

모델이 양성 판단을 매우 신중하게 잘하고 있다는 뜻

<br>

<mark>**0.5**</mark>

모델이 양성이라고 판단한 것중에 절반만 양성이다.

<br>

<mark>**즉, Precision이 낮을 수록 모델이 “양성이라고 해놓고 틀린 경우”가 많다는 뜻**</mark>

<br>

### 중요한 이유

특정 상황에서는 양성 예측이 틀린 경우 매우 큰 문제가 발생할 수 있다.

<mark>**예시1**</mark>

의료 진단에서 “암이 없는데 있다고 예측”(False Positive)이 발생하면 환자에게 불필요한 추가 검사나 수술이 발생할 수 있다.

<br>

<mark>**예시2**</mark>

금융 사기 탐지 시스템에서 “정상 거래를 사기라고 예측”(False Positive)하면 사용자가 불편을 겪을 수 있다.

<br>

## Recall(재현율)

<mark>**실제 양성인것들 중(TP + FN)**</mark>에서 <mark>**모델이 제대로 양성이라고 예측(TP)**</mark>한지 나타내는 수치

<br>

실제 양성인것들 중에서 → TP + FN

모델이 제대로 양성이라고 예측한 수치 → TP

$$
Recall = \frac{TP}{TP + FN}
$$

FN → 실제 양성인데 모델이 놓친것

<br>

### 중요한 이유

특정 상황에서 예측을 틀리면 안되는 경우가 있다.

<br>

예시1

암 진단에서 환자가 “암이 있는데 없다고” 판단하는 False Negative 가 발생하면 심각한 문제가 생긴다.

이런 경우는 Recall이 높은 모델이 훨씬 중요하다.

Recall은 <mark>**“놓치지 않는 능력”**</mark>을 뜻하기 때문에, 모델이 얼마나 민감하게 양성을 감지하는지 보여준다.

따라서 민감도 라는 Sensitivity 라는 표현을 쓰기도 한다.

<br>

### Recall이 낮은 경우

Recall이 낮으면 실제 모델이 찾아야하는 양성데이터를 많이 놓치고 있다는 뜻이다.

양성인 대상이 많은데도 불구하고 <mark>**자꾸 음성으로 판단해 무시**</mark>한다는 뜻이다.

<br>

### Recall이 높은 경우

모델이 실제로 양성인 데이터를 거의 빠짐없이 잘 찾아냈다는 것이다.

놓친게 거의 없다는 뜻이다.

<br>

Recall이 너무 높이기만 하면 문제가 생긴다.

모델이 뭔가 양성일 가능성이 조금이라도 보이면 전부 Positive로 예측할 수 있다.

<br>

예를들어) 스팸 메시지에서 스팸아닌 메일까지도 스팸으로 분류하게 되어 불편할 수 있다.

<br>

## Precision vs Recall Trade-off

Precision은 모델이 양성이라고 예측한 것중에서 실제로 맞춘 비율

Recall은 실제 양성인것중에 모델이 놓치지 않고 잘 맞춘 비율

- 모델이 실제 정답을 얼마나 맞췄는지

둘의 차이는 FP와 FN이다.

FP는 모델이 실수로 양성이라 잘못 판단한것

FN은 실제 양성인데 모델이 잘못 예측한것

<br>

### <mark>**Precision ↑**</mark> Recall <mark>**↓**</mark>

Precision이 높다는 뜻 → FP(Fasle Positive)가 적음 → 양성으로 착각하는 경우가 적음

Recall이 낮음 → FN(False Negative)가 높음 → 음성으로 착각 하는 경우가 많음

<br>

<mark>**모델이 양성이라고 예측한 경우가 매우 적고, 매우 조심스럽게 예측을 하는 상황**</mark>

쉽게 말해서 “이건 진짜 확실하다” 싶은것만 양성으로 예측, 나머지는 다 음성

<br>

Positive라고 예측한 것들 중에 <mark>**틀린게 적어서 Precision이 높다.**</mark>

실제로 양성인 데이터 중 <mark>**상당수를 놓치므로 Recall이 낮다.**</mark>

<br>

예시

범죄자 예측 시스템에서 모델이 너무 조심스러움.

아주 강력한 증거가 있을때만 “이 사람이 범죄자다”라고 판단하고 나머지는 범죄자가 아니라고 함.

이 모델은 범죄자인 사람들을 많이 놓침. 하지만 모델이 범죄자라고 예측하면 진짜일 확률 높음

즉, Precision은 높고 Recall은 낮음

<br>

### <mark>**Precision ↓**</mark> Recall <mark>**↑**</mark>

Precision이 낮음 → FP(False Positive)가 높음 → 양성으로 착각하는 경우가 높음.

Recall이 높음 → FN(False Negative)가 낮음 → 음성으로 착각하는 경우가 낮음.

<br>

Precision이 낮다는 것은 양성으로 착각하는 경우가 높다는 것인데,

의심 되는건 다 Positive로 예측하는 식으로 넓게 잡는 경우다.

<br>

따라서 Recall이 낮다는 것은 음성으로 착각하는 경우가 낮다는 것인데,

그렇다는 것은 이것저것 다 Positive로 판단하기에 실제 양성을 양성이라고 하는경우가 많다는 것이다.

<br>

“혹시라도 중요한걸 놓치면 안되니깐 의심되면 전부 양성이라고 해버리자” 전략이다.

<br>

예시

스팸 필터가 사용자가 절대로 스팸을 보게 하면 안된다는 기준이 있다.

아주 조금이라도 의심스러운 메일을 모두 스팸으로 분류한다.

실제 스팸은 거의 다 걸러져 Recall이 높다.

하지만 많은 일반 메일도 스팸함으로 보내게되어 Precision은 낮아진다.

즉, “많이 잡았지만 헛발질도 많았다” 라는 뜻이다.

<br>

### 장단점

두 상황은 모두 장단점이 분명하다.

상황에 따라 어떤것이 중요한지 판단해야한다.

Recall이 낮고 Precision이 높은 경우 모델이 신중하게 예측하고 틀릴 확률이 적다.

하지만 많은 양성 데이터를 놓치게 되는 상태이다.

Recall이 높고 Precision이 낮은 경우는 모델이 민감하게 반응해서 많은 양성을 잡는다.

하지만 정확도는 떨어져서 잘못된 판단이 많아지는 상태이다.

<br>

따라서 의료, 범죄, 화재 경보 처럼 놓치면 안되는 경우는 Recall이 더 중요하다.

법적 판단, 자동 필터링, 사용자에게 직접 영향을 주는 서비스는 Precision이 더 중요하다.

<br>

이 둘은 trade-off 관계에 있기때문에 둘다 동시에 높이는 것은 어렵다.

그 중간을 조율하기 위해 F1-score을 사용한다.

<br>

F1-score은 Precision과 Recall을 동시에 고려하는 수치이다.

그 수치가 높다는 것은 Precision과 Recall이 둘다 어느 정도 균형있게 높다는 뜻이다.

한쪽만 극단적으로 높은경우는 F1-score는 낮게 나온다.

<br>

### 정리

최종적으로 모델이 실제 상황에서 어떤 역활을 하는지에 따라 Precision과 Recall 중 어느 것을 중시할지 결정하고, 필요하면 F1-score을 기준으로 최적의 모델을 결정해야한다.
