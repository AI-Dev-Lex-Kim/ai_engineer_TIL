- [Backpropagation, 경사 하강법 업데이트 Deep Dive](#backpropagation-경사-하강법-업데이트-deep-dive)
- [Gradient Descent(경사 하강법)](#gradient-descent경사-하강법)
  - [딥러닝 미분, 편미분](#딥러닝-미분-편미분)
    - [∇W L](#w-l)
    - [∂](#)
  - [Backpropagation Gradient Descent 예시](#backpropagation-gradient-descent-예시)
    - [Forward pass(순전파)](#forward-pass순전파)
    - [Backpropagation](#backpropagation)
    - [ReLU 미분](#relu-미분)
    - [최종 출력](#최종-출력)

---

<br>

# Backpropagation, 경사 하강법 업데이트 Deep Dive

딥러닝 모델이 학습이 되는 구조는 아래와 같다.

가중치 행렬과 입력 x의 계산을 통해서 $\hat{y}$ 을 만든다.

$\hat{y} = f\_W(x)$

즉, $\hat{f}$는 W(가중치 행렬)에 따라서 달라진다.

<br>

이렇게 만든 $\hat{y}$와 정답인 $y$와의 차이를 계산한것이 loss이다.

$loss = L(\hat{y}, y)$

<br>

# Gradient Descent(경사 하강법)

loss를 최대한 줄이는것이 딥러닝 모델의 학습 방법이다.

새롭게 가중치 행렬 W를 업데이트 하는 방법은 아래와 같다.

$NewW = Cur_W - η * ∇W L$

η = learning rate

∇W L = 손실 함수를 사용해서 가중치 행렬 W에 대해 미분한 값

<br>

## 딥러닝 미분, 편미분

### ∇W L

∇는 나블라, nabla 라는 기호이다. 이건 딥러닝 기울기와 미분을 이해하는 기호이다.

일반적인 함수 미분은 다음과 같다.

$f(x)=x ^2 f ′ (x)=2x$

<br>

하지만 딥러닝에서는 입력이 벡터인 형식이 많아 각 벡터 요소에 미분한 값이 필요하다.

이 미분 값을 전부 모은게 미분 벡터, gradient 벡터라고 부른다.

$L = Loss(x_1, x_2, …, x_n)$

<br>

기호로 표현하면 ∇L(L)이 된다.

딥러닝에서는 가중치 행렬 W의 각 요소에 대해 미분을 진행한다. 그것을 $∇_W L$ 으로 표현한것이다.

$∇_W L =[[∂L/∂w₁₁, ∂L/∂w₁₂, ∂L/∂w₁₃], [∂L/∂w₂₁, ∂L/∂w₂₂, ∂L/∂w₂₃]]$

<br>

### ∂

∂은 partial derivative 기호, 편미분 기호, 델 이라고 부른다.

예시를 들어보자.

<br>

`f(x, y) = x² + 3xy + y²`은 입력이 x, y 두개이다.

각 x, y에 미분을 하면 아래은 수식을 사용한다.

`∂f/∂x = 2x + 3y`

함수 f(x, y)를 x에 대해서만 미분한것이다.(y는 고정)

<br>

x가 변했을때 함수 f의 값이 얼마나 바뀌는지 나타내는 식이다.

어떤 점 (x=1, y=2) 에서 x를 + 0.01 만큼 움직인다고 해보자.

`∂f/∂x at (1,2) = 2 * 1 + 3 * 2 = 2 + 6 = 8`

x를 0.01 증가시키면 f는 대략 8 \* 0.01 = 0.08 만큼 증가할 것이라는 의미이다.

<br>

`∂f/∂y = 3x + 2y`

함수 f(x, y)를 y에 대해서만 미분한다는 뜻이다. (x는 고정)

<br>

`∂L / ∂w₁₁` 이 표현이 뜻하는것은 아래와 같다.

가중치 행렬 $W{_1}{_1}$ (1,1)번째 원소가 아주 조금 변했을때, 손실 함수 L이 얼마나, 어떤 방향으로 변했느냐를 나타낸것이다.

예를 들어 $W{_1}{_1}$을 +0.0001만큼 증가시키면, L이 얼마나 증가하거나 감소하는지 알려주는 값이 이 편미분이다.

<br>

## Backpropagation Gradient Descent 예시

먼저 Chain Rule에 대해 알아야한다.

딥러닝 모델은 여러 함수의 조합으로 구성되어 있다.

그래서 그 함수들을 직접 미분할 수 없고 거쳐가는 중간 값들에 대해 하나씩 미분해서 연결해야한다.

이걸 **“연쇄법칙”**이라고 한다.

<br>

아래와 같은 예시로 이해해보자.

```python
import numpy as np

# 입력 x: shape (2, 1)
x = np.array([[1.0], [2.0]])

# 정답 y: shape (1, 1)
y = np.array([[1.0]])
# 1층 (입력 → 은닉층)
W1 = np.array([[0.1, 0.2],     # shape: (2, 2)
               [0.3, 0.4]])
b1 = np.array([[0.1], [0.2]])  # shape: (2, 1)

# 2층 (은닉층 → 출력층)
W2 = np.array([[0.5, 0.6]])    # shape: (1, 2)
b2 = np.array([[0.3]])         # shape: (1, 1)
```

x는 입력값

y는 정답 값

W1는 1층 은닉층 가중치 행렬

b1은 1층 편향 행렬

W2는 2층 은닉층 가중치 행렬

b2는 2층 편향 행렬

<br>

### Forward pass(순전파)

아래는 Forward pass(순전파)를 실행하는 과정이다.

```python
# 1층: z1 = W1 @ x + b1
z1 = W1 @ x + b1  # shape: (2, 1)
# ReLU
a1 = np.maximum(z1, 0)  # shape: (2, 1)

# 2층: z2 = W2 @ a1 + b2
z2 = W2 @ a1 + b2  # shape: (1, 1)

# 출력값: y_hat
y_hat = z2

# 손실함수: MSE = 0.5 * (y_hat - y)^2
loss = 0.5 * (y_hat - y) ** 2
```

선형방정식으로 W1와 b1을 통해 z1을 구한다.

z1을 ReLU 활성화 함수로 비선형 시켜준다.

선형방정식으로 W2와 b2를 통해 z2를 구한다.

loss를 z2인 y_hat과 정답 y를 계산해 구해준다.

```python
x = np.array([[1.0], [2.0]])

# 정답 y: shape (1, 1)
y = np.array([[1.0]])
# 1층 (입력 → 은닉층)
W1 = np.array([[0.1, 0.2],     # shape: (2, 2)
               [0.3, 0.4]])
b1 = np.array([[0.1], [0.2]])  # shape: (2, 1)

# 2층 (은닉층 → 출력층)
W2 = np.array([[0.5, 0.6]])    # shape: (1, 2)
b2 = np.array([[0.3]])         # shape: (1, 1)
```

결과는 아래와 같이 나온다.

```python
"""
W1 = [[0.1, 0.2],   # shape: (2, 2)
      [0.3, 0.4]]
x = [[1.0], [2.0]]
b1 = [[0.1], [0.2]]
"""
# z1 = W1 @ x + b1  # shape: (2, 1)
z1 = [[0.1*1 + 0.2*2 + 0.1] = 0.6]
     [[0.3*1 + 0.4*2 + 0.2] = 1.3]

# ReLU
# a1 = np.maximum(z1, 0)  # shape: (2, 1)
a1 = ReLU(z1) = [[0.6], [1.3]]

"""
W2 = [[0.5, 0.6]]
b2 = [[0.3]]
"""
# z2 = W2 @ a1 + b2  # shape: (1, 1)
z2 = 0.5*0.6 + 0.6*1.3 + 0.3 = 0.3 + 0.78 + 0.3 = 1.38

# 출력값: y_hat = z2
ŷ = [[1.38]]

# 손실함수: MSE = loss = 0.5 * (y_hat - y) ** 2
loss = 0.5 * (1.38 - 1.0)^2 = 0.5 * 0.1444 = 0.0722
```

<br>

### Backpropagation

가중치와 편향을 학습 하기위한 편미분값을 얻어야한다.

- ∂L/∂W2 → 손실함수 L에 대한 W2 편미분
- ∂L/∂b2 → 손실함수 L에 대한 b2 편미분
- ∂L/∂W1 → 손실함수 L에 대한 W1 편미분
- ∂L/∂b1 → 손실함수 L에 대한 b1 편미분

<br>

이건 모두 손실 함수 L을 각층의 가중치에 대해 편미분한 값이다.

즉, 가중치를 얼마나 바꾸면 L이 얼마나 변하는지 알고싶은 것이다.

하지만 이 값들은 L이 직접적으로 W2나 W1을 사용하는 함수가 아니여서 직접 미분이 불가능하다.

$loss =0.5 * (\hat{y} - y)^2$

<br>

딥러닝 모델은 중첩함수 구조이다.

`L = f(g(h(k(W))))` 같은 다중 중첩 함수 구조이다.

```python
x → z1 = W1 @ x + b1
     → a1 = ReLU(z1)
          → z2 = W2 @ a1 + b2
               → y_hat = z2
                    → L = loss(y_hat, y)
```

L은 W2에 대해 직접적인 함수가 아니고 중간 단계를 거쳐서 영향을 받는다.

그래서 연쇄법칙을 사용해야한다.

<br>

∂L/∂W2을 예로 들어 보자.

W2는 어디에 쓰이냐면 `z2 = W2 @ a1 + b2` 에서 쓰인다.

∂L/∂W2는 다음과 같은 경로로 영향을 받는다. W2 → z2 → $\hat{y}$ → L

이 경로를 따라서 **미분값들을 곱하면** ∂L/∂W2이 된다.

<br>

곱하는 이유는 아래의 예시를 보면 이해할 수 있다.

- L이 y에 얼마나 민감한가? → ∂L/∂y
- y가 z에 얼마나 민감한가? → ∂y/∂z
- z가 x에 얼마나 민감한가? → ∂z/∂x

이걸 하나의 사슬처럼 연결해서 보면

- x가 1만큼 바뀌면 z가 ∂z/∂x 만큼 바뀜
- z가 그렇게 바뀌면 y가 ∂y/∂z 만큼 바뀜
- y가 그렇게 바뀌면 L이 ∂L/∂y 만큼 바뀜

즉, 변화율은 각 단계마다 누적되면서 곱해지는것이다.

chain rull이 곱셈으로 표현되는 것이다.

<br>

딥러닝도 똑같다.

```python
z2 = W2 @ a1 + b2
ŷ = z2
L = 0.5 * (ŷ - y)²
```

W2는 z2를 만들고

z2는 ŷ가 되고

ŷ는 L을 만든다

그것은 chain rull을 통해 편미분을 하면 다음과 같이 해석된다.

<br>

**W2를 아주 약간 바꿨을 때**,

→ z2가 얼마나 바뀌는가? ∂z2/∂W2

**z2가 그렇게 바뀌면**,

→ ŷ가 얼마나 바뀌는가? ∂ŷ/∂z2

**ŷ가 그렇게 바뀌면**,

→ L이 얼마나 바뀌는가? ∂L/∂ŷ

<br>

이 전체적 변화율이 연쇄적으로 누적돼서

결국 W2가 조금 바뀌었을때 L이 얼마나 바뀌는지 알려주는 값이 된다.

<br>

다시 돌아와서 W2가 L에 영향을 미치는 순서는 다음과 같다.

W2 → z2 → $\hat{y}$ → L

따라서 ∂L/∂W2을 구하려면 chain rull을 통해 역방향으로 따라가면서 미분값을 곱해준다.

이것을 우리는 Backpropagation(역전파) 라고 부른다.

```python
∂L/∂W2 = ∂L/∂ŷ * ∂ŷ/∂z2 * ∂z2/∂W2
```

미분순서는 출력값 부터 시작한다.

L → $\hat{y}$ → z2 → W2

1. ∂L/∂ŷ

```python
L = 0.5 * (ŷ - y)^2
-> ŷ - y
```

<br>

1. ∂ŷ/∂z2

```python
ŷ = z2
∂ŷ/∂z2 = 1
```

1. ∂z2/∂W2

```python
W2 = [[w1, w2, w3]]       (1×3)
a1 = [[a1],
      [a2],
      [a3]]               (3×1)

z2 = W2 @ a1 + b2

∂z2/∂W2 = [[∂z2/∂w1, ∂z2/∂w2, ∂z2/∂w3]]

z2 = w1*a1 + w2*a2 + w3*a3
∂z2/∂w1 = a1
∂z2/∂w2 = a2
∂z2/∂w3 = a3

즉, 아래와 같다.
∂z2/∂W2 = [[a1, a2, a3]]

이건 a1을 행벡터로 변환한것, a1.T와 같다.

결론은 ∂z2/∂W2 = a1.T 라는것이다.
```

<br>

이제 이 3개를 곱해주자.

```python
∂L/∂W2 = ∂L/∂ŷ * ∂ŷ/∂z2 * ∂z2/∂W2
			 = (ŷ - y) * 1 * a1.T
			 = 2.0 * 1 * [[3.0], [4.0]]
			 = [[6.0, 8.0]]
```

이게 W2의 각 요소에 대한 미분값이다.

즉, $W2{_2}{_1}$을 0.01 만큼 바꾸면 L은 0.06만큼 바뀐다.

<br>

이제 나머지도 이어서 구해보자.

출력층 부터 시작

```python
# dL/dy_hat = ŷ - y
# dL/dŷ = 1.38 - 1.0 = 0.38
dL_dyhat = y_hat - y

# dL/dz2 = dL/dy_hat = 0.38
dz2 = dL_dyhat  # shape: (1, 1)

a1 = [[0.6], [1.3]]

# dL/dW2 = dz2 @ a1.T
# dW2 = 0.38 * [0.6, 1.3] = [[0.228, 0.494]]
dW2 = dz2 @ a1.T

# dL/db2 = dz2 = db2 = [[0.38]]
db2 = dz2  # shape: (1, 1)
```

<br>

은닉층 역전파

```python
# dL/da1 = W2.T @ dz2
# W2 = [[0.5], [0.6]]
# da1 = W2.T @ dz2 = [[0.5], [0.6]]^T @ 0.38 = [[0.19], [0.228]]
da1 = W2.T @ dz2  # shape: (2, 1)

z1 = [[0.6], [1.3]] → ReLU 활성화 통과했으므로 모두 양수 → ReLU' = 1

# dz1 = da1 * 1 = [[0.19], [0.228]]
# ReLU 미분: z1 > 0 인 부분만 살림
dz1 = da1 * (z1 > 0).astype(float)  # shape: (2, 1)

"""
x = [[1.0], [2.0]] → x.T = [[1.0, 2.0]]

dW1 = dz1 @ x.T = [[0.19], [0.228]] @ [[1.0, 2.0]]
     = [[0.19*1.0, 0.19*2.0], [0.228*1.0, 0.228*2.0]]
     = [[0.19, 0.38], [0.228, 0.456]]
"""
dW1 = dz1 @ x.T  # shape: (2, 2)

# dL/db1 = dz1
# db1 = dz1 = [[0.19], [0.228]]
db1 = dz1  # shape: (2, 1)
```

<br>

### ReLU 미분

ReLU 미분은

- 입력값이 0 보다 크면 1
- 입력값이 0보다 작거나 같으면 0이다.

<br>

예를 들면

```python
z = [-2.0, 0.0, 3.0, -1.5, 2.0]
a1 = torch.relu(z)
```

여기서 ∂a1/∂z을 하게 된다면 다음과 같다.

```python
(z1 > 0).astype(float)
```

(z1 > 0 )은 각 요소가 0보다 큰지 비교해서 True/False 값들로 이루어진 배열을 만든다.

.astype(float)은 그 True/False 값을 숫자로 바꿔준다.

True → 1.0

False → 0.0

```python
z = [-2.0, 0.0, 3.0, -1.5, 2.0]
->
∂a1/∂z = (z1 > 0).astype(float)
			 = [False, False, True, False, True]
			 = [0, 0, 1, 0, 1]
```

<br>

### 최종 출력

```python
dW2 = [[0.228 0.494]]
db2 = [[0.38]]
dW1 = [[0.19  0.38 ]
       [0.228 0.456]]
db1 = [[0.19 ]
       [0.228]]
```

손실 L(ŷ, y)가 각 층의 가중치 W에 대해 어떻게 변하는지를 알고 싶다.

- `dW2 = [[0.228, 0.494]]`: 출력층의 가중치를 이만큼 줄이면 손실이 감소한다.
- `dW1 = [[0.19, 0.38], [0.228, 0.456]]`: 입력층 쪽 W도 이만큼 수정해야 손실이 줄어든다.

<br>

W1, W2의 각 원소에 대해 다음처럼 해석할 수 있다.

`∂L/∂W1[0][0] = 0.19`

→ "W1의 (0,0) 요소를 0.0001만큼 증가시키면, 손실 L이 대략 0.000019만큼 증가한다"는 의미이다.

즉, 모든 가중치의 변화량(gradient)을 얻었고, 이를 사용해 경사하강법으로 가중치를 업데이트할 수 있다.

<br>

파라미터 업데이틑 아래와 같이 진행한다.

```python
lr = 0.1
W2 -= lr * dW2
b2 -= lr * db2
W1 -= lr * dW1
b1 -= lr * db1
```

손실함수 L을 줄이기 위해서,

그 함수가 가장 급격히 감소하는 방향(기울기의 음의 방향)으로 이동해야한다.

즉, gradient는 함수가 가장 빠르게 증가하는 방향이다.

그 반대 방향으로 이동해서 손실 값을 줄여야 하니깐 빼줘야한다.
