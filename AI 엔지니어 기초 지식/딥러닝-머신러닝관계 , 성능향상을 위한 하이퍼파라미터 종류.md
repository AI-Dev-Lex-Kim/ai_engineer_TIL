## 딥러닝과 머신러닝 간의 포함관계에 대해 설명해주세요.

우리는 인공지능이라는 거대한 기술 분야 속에서 여러 개념들이 서로 겹치고 확장되며 발전해 왔다는 사실을 이해해야 합니다. 그리고 그 안에서 자주 혼동되는 두 용어, **머신러닝**과 **딥러닝**의 포함관계는 단순한 지식 정리를 넘어, 인공지능 시스템을 이해하고 설계하는 데 핵심적인 출발점이 됩니다. 특히 산업계나 연구 현장에서는 이 두 개념을 구분하지 않고 사용하는 경우도 많지만, 실제로 이들 사이에는 **명확한 계층적 포함관계**가 존재합니다. 이 포함관계를 이해하는 것은 단지 용어 정리 차원의 문제가 아니라, **각 기술의 특성과 역할, 활용 방식이 어떻게 다른지를 명확히 구분짓고**, 앞으로 어떤 모델을 선택하고 적용할지에 대한 실질적인 판단 기준을 세우는 데 매우 중요합니다.

전체 구조를 큰 틀에서 보면, 인공지능(AI)이라는 가장 넓은 영역이 존재하고, 그 안에 머신러닝(Machine Learning)이라는 기술 집합이 존재하며, 다시 머신러닝의 하위 범주로서 딥러닝(Deep Learning)이 존재합니다. 즉, **딥러닝은 머신러닝의 일부이고, 머신러닝은 인공지능의 일부**입니다. 이처럼 계층적으로 포함되어 있는 구조를 이해하면, 어떤 문제에 대해 AI 기술을 적용할 때 우리가 어디에 속한 기술을 쓰고 있는지를 명확히 파악할 수 있습니다.

가장 바깥쪽에는 **AI, 인공지능**이 있습니다. 인공지능은 사람이 수행할 수 있는 지적인 작업, 즉 **판단, 예측, 추론, 의사결정, 학습, 문제 해결**과 같은 능력을 컴퓨터가 흉내 내거나 모방할 수 있도록 하는 기술 전반을 뜻합니다. 인공지능의 개념은 수십 년 전부터 존재해 왔으며, 초기에는 규칙 기반(rule-based) 시스템처럼 **사람이 논리 규칙을 직접 만들어주는 방식**으로 작동했습니다. 예를 들어, 체스에서 ‘이 상황에서는 이렇게 움직여라’ 같은 수많은 규칙을 사람이 일일이 짜는 방식입니다. 하지만 이 접근은 복잡한 문제에 직면할수록 너무 많은 규칙이 필요해지고, 변화에 적응하지 못한다는 단점이 있었습니다.

이러한 한계를 넘어서기 위해 등장한 것이 **머신러닝**, 즉 **기계 학습**입니다. 머신러닝은 사람이 규칙을 다 짜주지 않고, **데이터로부터 스스로 규칙을 학습**하게 하자는 발상에서 출발합니다. 즉, ‘정답이 있는 데이터’를 주면 모델이 스스로 규칙을 찾아내는 방식입니다. 예를 들어, 여러 개의 고양이 사진을 보여주면서 이것은 고양이라고 알려주면, 머신러닝 모델은 ‘고양이라는 이미지는 어떤 패턴을 가지고 있구나’라는 것을 스스로 배웁니다. 이렇게 스스로 규칙을 만들어내는 것이 머신러닝의 핵심이고, 이것이 AI가 좀 더 유연하고 확장성 있게 진화할 수 있었던 중요한 전환점이었습니다.

하지만 머신러닝도 또 다른 문제를 안고 있었습니다. 그것은 바로 **피처(Feature, 특징) 추출의 어려움**이었습니다. 대부분의 머신러닝 알고리즘은 입력 데이터를 바로 사용하지 못하고, 사람이 먼저 데이터를 분석해서 중요한 특징을 뽑아주는 ‘피처 엔지니어링(feature engineering)’을 거쳐야 했습니다. 예를 들어, 고양이 이미지를 분류하려면 사람 눈으로 봤을 때 귀의 크기, 눈의 간격, 털 색깔 같은 것들을 사람이 정의해주어야 모델이 그걸 학습할 수 있었습니다. 즉, 여전히 사람의 수작업이 필요했고, 그 수작업이 성능에 큰 영향을 미쳤습니다.

이 지점을 뛰어넘기 위해 나온 기술이 바로 딥러닝(Deep Learning)입니다. 딥러닝은 사람이 피처를 직접 뽑지 않아도, **데이터만 넣으면 모델이 스스로 특징을 뽑아내고, 그걸 학습까지 하는 기술**입니다. 이것이 가능해진 것은 인공신경망이라는 구조 덕분입니다. 인공신경망은 사람 뇌의 뉴런 구조를 모방하여 데이터를 계층적으로 처리하는 방식인데, 여기에 층을 깊게 쌓으면서 복잡한 표현을 학습할 수 있게 된 것이 바로 딥러닝입니다. 즉, 딥러닝은 **많은 층을 가진 인공신경망을 사용한 머신러닝** 기술이라고 할 수 있고, 따라서 자연스럽게 머신러닝의 하위 범주에 포함됩니다.

즉, 머신러닝은 딥러닝보다 더 넓은 개념입니다. 머신러닝에는 전통적인 알고리즘들, 예를 들어 **서포트 벡터 머신(SVM)**, **의사결정트리(Decision Tree)**, **K-최근접 이웃(K-NN)**, **랜덤포레스트(Random Forest)** 등도 포함됩니다. 이들은 주로 피처 엔지니어링에 의존하며, 입력을 어떤 방식으로 가공하느냐에 따라 성능이 달라지곤 했습니다. 반면 딥러닝은 피처를 스스로 뽑아내며, CNN(합성곱 신경망), RNN(순환 신경망), Transformer 같은 다양한 딥러닝 모델들이 특정 문제에 특화되어 사용됩니다.

이러한 포함관계를 시각적으로 나타내면 다음과 같습니다:

```
인공지능 (AI)
└── 머신러닝 (ML)
    └── 딥러닝 (DL)

```

이 포함관계는 단순한 도식이 아니라, **각 기술이 문제를 푸는 방식과 설계 철학이 어떻게 발전해왔는지를 보여주는 역사이자 구조**입니다. AI는 처음에는 사람처럼 생각하게 만드는 것이 목적이었지만, 그 수단으로 머신러닝이라는 자동화된 학습 방법이 사용되었고, 그 머신러닝이 사람의 손을 덜 빌리기 위해 딥러닝이라는 자기 표현 학습 구조로 발전해온 것입니다.

마지막으로 정리하자면, 딥러닝은 단지 '더 깊은 네트워크'를 사용하는 것이 전부가 아니라, 머신러닝이 안고 있던 **특징 추출의 한계**, **복잡한 데이터 처리의 어려움**, **모델 일반화 성능 향상** 같은 문제들을 해결하기 위한 구조적 진화의 결과이며, 따라서 단순히 머신러닝의 일종이라기보다, **현대 머신러닝의 주력 기술**로 자리 잡은 매우 강력한 하위 분류입니다. 하지만 분명한 것은, 딥러닝이 머신러닝의 일부이고, 머신러닝은 인공지능의 일부라는 계층 구조는 이론적으로나 실무적으로나 여전히 유효하다는 점입니다.

<br>

## 딥러닝의 성능향상을 위해 고려하는 하이퍼파라미터의 종류에는 어떤 것들이 있는지 설명해주세요.

딥러닝은 말 그대로 데이터를 이용해 모델이 스스로 문제를 해결할 수 있는 규칙이나 패턴을 학습하게 만드는 구조입니다. 이때 학습의 핵심은 모델이 데이터를 보고, 정답을 얼마나 잘 맞추는지를 평가하면서 그 예측 능력을 점점 높여가는 것입니다. 이를 위해 우리는 손실 함수라고 불리는 수식을 통해 예측이 얼마나 틀렸는지를 수치로 표현하고, 그 수치를 줄여가도록 모델 내부의 가중치와 편향이라는 값을 조금씩 조정합니다. 이 과정을 가능하게 해주는 핵심 구성 요소가 바로 옵티마이저입니다.

옵티마이저는 딥러닝 모델이 손실을 줄이기 위해 **가중치를 어떤 방향으로, 얼마만큼 조정해야 할지를 계산해주는 알고리즘**입니다. 가중치는 딥러닝 모델의 각 층에서 입력값과 출력값을 연결해주는 핵심 변수로, 이 값이 바뀌면 모델의 예측 결과도 달라지게 됩니다. 손실 함수가 계산된 뒤에는, 손실 값을 줄이기 위해 손실 함수의 기울기, 즉 미분값을 계산하는데, 이 기울기가 바로 우리가 어느 방향으로, 얼마나 이동해야 손실이 줄어들지를 알려줍니다. 이러한 기울기 정보를 받아 가중치를 실제로 업데이트하는 주체가 바로 옵티마이저입니다.

가장 기본적인 옵티마이저는 확률적 경사 하강법, 즉 Stochastic Gradient Descent인데, 이는 훈련 데이터에서 일부 샘플만을 사용해서 대략적인 기울기를 계산하고 그 방향으로 조금씩 가중치를 조정하는 방식입니다. 이때 기울기를 얼마나 강하게 적용해서 가중치를 조정할지 결정하는 것이 바로 학습률이라는 하이퍼파라미터입니다. 학습률이 크면 빠르게 손실이 줄어들지만 너무 크면 오히려 최적점 근처에서 튕겨 나갈 수 있고, 너무 작으면 학습이 지나치게 느려질 수 있습니다.

이제 옵티마이저가 가중치를 조정하는 역할이라는 것을 이해했으니, 이 옵티마이저가 학습을 수행할 때 어떤 단위로 학습을 반복하고, 그 반복이 전체 학습에서 어떤 위치를 차지하는지를 이해해야 합니다. 그와 관련된 두 가지 개념이 바로 에폭(epoch)과 이터레이션(iteration)입니다.

에폭은 전체 학습 데이터셋을 한 번 전부 모델에 통과시켜 학습을 완료하는 것을 의미합니다. 예를 들어, 1만 개의 이미지를 가지고 학습을 한다면, 이 1만 개를 전부 모델이 한 번 학습하는 것을 1 에폭이라고 합니다. 하지만 이 1만 개를 한꺼번에 GPU에 올려서 학습하는 것은 불가능에 가깝기 때문에, 이 데이터를 여러 개의 조각으로 나누어 조금씩 모델에 넣게 됩니다. 이 조각 하나하나를 배치(batch)라고 하고, 이때의 크기를 배치 사이즈라고 부릅니다. 예를 들어 배치 사이즈가 100이라면, 1만 개의 데이터를 100개씩 나누어 총 100번 모델에 넣어줘야 비로소 한 에폭이 완료됩니다.

이터레이션은 바로 그 한 번의 배치가 모델에 들어가고, 손실이 계산되고, 가중치가 조정되는 일련의 과정이 완료된 것을 뜻합니다. 즉, 1 이터레이션은 한 배치를 가지고 모델이 학습을 한 번 수행한 것을 의미하며, 총 이터레이션 수는 에폭 수와 배치 수의 곱으로 계산됩니다. 따라서 에폭이 10이고, 배치 수가 100이면 총 1,000번의 이터레이션이 발생한 것입니다.

이제 모델이 반복적으로 학습을 하게 되면 점점 손실이 줄어들고 성능이 올라가게 되지만, 어떤 순간부터는 오히려 성능이 나빠질 수 있습니다. 훈련 데이터에는 매우 잘 맞는데, 훈련에 쓰지 않은 새로운 데이터에는 잘 맞지 않는 현상이 발생하기 때문입니다. 이것을 우리는 과적합(overfitting)이라고 부르며, 모델이 너무 오래, 너무 복잡하게 학습했을 때 발생합니다. 이를 방지하기 위한 기법 중 하나가 바로 드롭아웃(dropout)입니다.

드롭아웃은 학습 도중 인공신경망의 일부 뉴런을 무작위로 제거(drop)해서 계산에서 제외하는 방식입니다. 이는 마치 팀 프로젝트를 할 때 어떤 사람이 랜덤하게 자리를 비우는 것과 같아서, 남아있는 사람들이 각자 더 많은 책임을 지게 되는 효과를 만들어냅니다. 이렇게 함으로써 모델이 일부 뉴런에 과도하게 의존하는 것을 막고, 보다 일반화된 학습을 하도록 유도합니다. 드롭아웃은 학습할 때만 적용되며, 실제 추론 시에는 전체 뉴런을 모두 사용하되 드롭아웃의 확률을 반영하여 보정된 출력을 사용합니다. 드롭아웃의 확률, 즉 어느 정도 비율로 뉴런을 꺼버릴지에 대한 값 역시 하이퍼파라미터로 사람이 설정해줘야 합니다.

이처럼 과적합은 모델이 너무 훈련 데이터에만 맞춰 학습하게 되었을 때 발생하는 문제인데, 이를 감지하고 자동으로 멈추게 하는 방법이 또 하나 있습니다. 그것이 바로 조기 종료(early stopping)입니다. 조기 종료는 이름 그대로, 학습을 도중에 멈추는 방법인데 단순히 에폭을 미리 정해놓고 멈추는 것이 아니라, 훈련 도중 검증 데이터(validation set)에 대한 성능을 지속적으로 모니터링하면서, 일정 시간 동안 성능이 더 이상 좋아지지 않으면 학습을 중단하는 방식입니다. 이렇게 하면 모델이 아직 완전히 과적합에 빠지기 전에 학습을 종료하여, 가장 일반화 성능이 좋은 시점의 모델을 얻을 수 있습니다. 이때 어느 정도 성능 향상이 없을 때 멈출지, 그 기준이 되는 기간도 하이퍼파라미터로 설정할 수 있습니다. 예를 들어 5번 연속으로 검증 정확도가 오르지 않으면 멈춘다는 식입니다.

지금까지 설명한 옵티마이저, 에폭과 이터레이션, 드롭아웃, 조기 종료는 모두 **딥러닝 모델이 얼마나 잘 학습되고 일반화되었는지를 결정하는 데 핵심적인 요소들**입니다. 이 개념들이 하이퍼파라미터 설명에서 반복적으로 등장하기 때문에, 이들에 대한 선이해가 되어 있어야 하이퍼파라미터 종류에 대한 설명이 끊기지 않고 이어질 수 있습니다.
