- [가설함수](#가설함수)
  - [선형 회귀에서 계수는 무엇을 의미할까?](#선형-회귀에서-계수는-무엇을-의미할까)
    - [**최적선이란?**](#최적선이란)
    - [**4-2. 최적선을 찾는 과정**](#4-2-최적선을-찾는-과정)
  - [**4. 가설 함수를 사용해서 어떻게 최적선을 구해?**](#4-가설-함수를-사용해서-어떻게-최적선을-구해)
    - [**4-1. 최적선이란?**](#4-1-최적선이란)
    - [**4-2. 최적선을 찾는 과정**](#4-2-최적선을-찾는-과정-1)
    - [**4-3. 왜 가설 함수가 필요할까?**](#4-3-왜-가설-함수가-필요할까)
    - [손실 함수란?](#손실-함수란)
  - [**3. 그래서 반복해서 세타값을 변경해서 그중에서 최솟값이 가장 최적이라는 건가?**](#3-그래서-반복해서-세타값을-변경해서-그중에서-최솟값이-가장-최적이라는-건가)
    - [**3-1. 최적의 세타 값 찾는 과정**](#3-1-최적의-세타-값-찾는-과정)
  - [**1. 가설 함수(hypothesis function)란?**](#1-가설-함수hypothesis-function란)
    - [**미분이 중요한 이유?**](#미분이-중요한-이유)
    - [미분을 사용한 예시](#미분을-사용한-예시)
    - [예시 1: $f(x)=f(x) = x^2$](#예시-1-fxfx--x2)
    - [**기울기(Gradient)**](#기울기gradient)
    - [**경사하강법(Gradient Descent)**](#경사하강법gradient-descent)
    - [**기울기와 경사하강법의 관계**](#기울기와-경사하강법의-관계)
    - [기울기 계산 예시 (손실 함수 예시)](#기울기-계산-예시-손실-함수-예시)
  - [편미분을 하는 이유](#편미분을-하는-이유)
    - [1. 다변수 함수에서 각 변수의 기울기가 중요한 이유](#1-다변수-함수에서-각-변수의-기울기가-중요한-이유)
    - [예시](#예시)
    - [2. 다변수 함수에서 기울기를 구하는 이유](#2-다변수-함수에서-기울기를-구하는-이유)
    - [(1) **변수 간의 관계 파악**](#1-변수-간의-관계-파악)
    - [(2) **최적화 및 경사하강법**](#2-최적화-및-경사하강법)
    - [(3) **함수의 기울기를 1개만 구하기 어려움**](#3-함수의-기울기를-1개만-구하기-어려움)
    - [(4) **다변수 함수에서 모든 변수의 변화율을 알기 위해**](#4-다변수-함수에서-모든-변수의-변화율을-알기-위해)
    - [3. 기울기를 여러 개 구하면 어떤 일이 발생할까?](#3-기울기를-여러-개-구하면-어떤-일이-발생할까)
  - [정리](#정리)

---

<br>

# 가설함수

<br>

## 선형 회귀에서 계수는 무엇을 의미할까?

선형 회귀에서는 계수를 $\theta$라고 표현한다.

예제)

$$
h(x)=θ_0+θ_1x_1+θ_2x_2
$$

- $θ1​ : x1x_1x1$의 계수 → 집 크기의 영향력
- $θ2​ : x2x_2x2$의 계수 → 방 개수의 영향력
- $\theta_0 :$ 상수항 (절편)
  <br>
  즉,

**각 입력 변수 $xi​$가 최종 결과에 얼마나 영향을 주는지를 나타내는 값** 이 바로 계수이다.
<br>

### **최적선이란?**

우리가 원하는 것은 **주어진 데이터에 가장 잘 맞는 직선**이다.

하지만 우리가 직접 $\theta_0​, \theta_1$을 정할 수는 없다.

따라서 **최적의 $\theta$ 값을 찾아야 한다**.
<br>

### **4-2. 최적선을 찾는 과정**

1. **임의의 가설 함수 설정**

$$
h(x)=\theta_0+\theta_1x
$$

- 처음에는 $\theta_0$와 $\theta_1$값을 아무렇게나 정한다.
- **오차(손실) 측정**
  우리가 찾은 선이 얼마나 잘 맞는지 확인해야 한다. 이를 위해 **오차(error)를 측정**한다.

$$
오차=실제값−예측값
$$

- **오차를 최소화하는 방향으로 $\theta$를 조정**
  우리가 만든 직선이 데이터에 잘 맞도록 **$\theta_0, \theta_1$ 값을 조금씩 변경**한다.
  이를 위해 손실 함수(loss function)를 사용하여 오차를 최소화하는 방향으로 값을 업데이트한다.
  가장 많이 쓰이는 방법이 경사 하강법(Gradient Descent)이다.
- **반복하여 최적의 $\theta$ 찾기**
  오차가 최소가 될 때까지 계속해서$\theta$ 값을 조정하여 최적선을 찾는다.

<br>

## **4. 가설 함수를 사용해서 어떻게 최적선을 구해?**

단순히 **일차 함수 하나를 정한다고 최적선이 되는 것이 아니다**.

<br>

### **4-1. 최적선이란?**

우리가 원하는 것은 **주어진 데이터에 가장 잘 맞는 직선**이다.

하지만 우리가 직접 $\theta_0\theta_1$을 정할 수는 없다.

따라서 **최적의 $\theta$ 값을 찾아야 한다**.

<br>

### **4-2. 최적선을 찾는 과정**

1. **임의의 가설 함수 설정**

   $$
   h(x)=θ0+θ1x
   $$

   처음에는θ0와 θ1 값을 아무렇게나 정한다.

2. **오차(손실) 측정**

우리가 찾은 선이 얼마나 잘 맞는지 확인해야 한다. 이를 위해 **오차(error)를 측정**한다.

$$
오차=실제값−예측값
$$

1. **오차를 최소화하는 방향으로 $\theta$를 조정**

   우리가 만든 직선이 데이터에 잘 맞도록 **$\theta_0,\theta_1$ 값을 조금씩 변경**한다.

   이를 위해 손실 함수(loss function)를 사용하여 오차를 최소화하는 방향으로 값을 업데이트한다.

   가장 많이 쓰이는 방법이 경사 하강법(Gradient Descent)이다.

2. **반복하여 최적의 $\theta$ 찾기**

   오차가 최소가 될 때까지 계속해서 $\theta$ 값을 조정하여 최적선을 찾는다.

<br>

### **4-3. 왜 가설 함수가 필요할까?**

가설 함수가 없다면 데이터를 설명할 수 있는 **기준(공식)이 없기 때문**이다.

가설 함수를 먼저 설정하고, 이를 최적화하는 방식으로 진행해야 데이터를 가장 잘 설명하는 최적선을 찾을 수 있다.

<br>

### 손실 함수란?

손실 함수는 **예측값과 실제값의 차이(오차)를 측정하는 함수**이다.

즉, **우리 모델이 얼마나 틀렸는지 수치적으로 평가하는 함수**라고 보면 된다.

선형 회귀에서 가장 많이 쓰는 손실 함수는 평균 제곱 오차(Mean Squared Error, MSE)이다.

$$
MSE=n1i=1∑n(예측값−실제값)2
$$

- 예측값이 실제값과 많이 다를수록 손실 함수 값이 커진다.
- 예측값이 실제값과 가까울수록 손실 함수 값이 작아진다.
- 우리가 해야 할 일은 **이 손실 함수 값을 최소화하는 $θ$ 값을 찾는 것**이다.

<br>

## **3. 그래서 반복해서 세타값을 변경해서 그중에서 최솟값이 가장 최적이라는 건가?**

정확하다!

### **3-1. 최적의 세타 값 찾는 과정**

1. **처음에는 아무 세타나 설정**

$$
h(x)=θ0+θ1x
$$

처음에는$θ0​,θ1$

- 을 랜덤하게 설정한다.
- **현재 세타 값에서 손실 함수 값 계산**
  - $\theta_0, \theta_1$을 사용해 예측값을 계산한다.
  - 예측값과 실제값의 차이를 구해 손실 함수 값을 계산한다.
  - **손실 함수 값을 줄이기 위해 세타 값을 업데이트**
    - **경사 하강법(Gradient Descent)** 같은 최적화 알고리즘을 사용해서 $θ$ 값을 조금씩 조정한다.
    - 목표는 손실 함수 값이 점점 줄어들도록 하는 것이다.
    1. **손실 함수 값이 최소가 될 때까지 반복**
       - 계속 반복해서 세타 값을 조정하면서 손실 함수 값을 줄인다.
       - 손실 함수 값이 더 이상 줄어들지 않으면, 그때의 세타 값이 최적이다.
         이 과정을 거쳐 **가장 오차가 작은 세타 값**을 찾게 된다.
         즉, **손실 함수가 최솟값을 가질 때의 세타 값이 가장 최적의 세타 값**이다.

<br>

## **1. 가설 함수(hypothesis function)란?**

가설 함수는 주어진 입력 변수(독립 변수)를 이용하여 **출력 값(종속 변수)을 예측하는 함수**이다.

$$
h(x)=θ0+θ1x
$$

### **미분이 중요한 이유?**

미분을 통해 **순간기울기**를 구할 수 있기 때문에, 함수가 **어떻게 변하는지**를 이해할 수 있습니다. 경사하강법에서 미분을 사용하는 이유도 이와 비슷합니다. 모델을 학습시킬 때 손실 함수(Loss Function)가 얼마나 빠르게 변화하는지 알아야 **어디로 가야 최솟값을 찾을 수 있을지** 알 수 있기 때문입니다.

<br>

예를 들어, 손실 함수의 **기울기가 0에 가까워진다**는 것은 그 지점이 **최소값**에 가까워지고 있다는 의미입니다. 그래서 미분을 통해 손실 함수의 기울기를 계산하고, 그 기울기를 **반영**하여 모델을 점점 더 정확하게 만들어가는 과정이 **경사하강법**입니다.

<br>

### 미분을 사용한 예시

<br>

### 예시 1: $f(x)=f(x) = x^2$

$f(x)=f(x) = x^2$라는 함수가 있을 때, 이 함수는 **곡선**이고, 각 점마다 기울기가 다릅니다. 예를 들어, x=1에서의 기울기를 알고 싶다면, 그때의 순간기울기를 미분으로 구한다.

1. **미분 구하기**:

$$
⁍
$$

$$
f′(1)=2(1)=2
$$

**x=1에서의 순간기울기**는 **2**입니다.

---

<br>

**기울기**와 **경사하강법**은 **최적화**와 관련이 있다. 경사하강법을 이해하기 위해서 기울기가 왜 중요한지, 그리고 경사하강법에서 기울기가 중요하다.

<br>

### **기울기(Gradient)**

기울기(혹은 기울기 벡터)는 함수가 **변화하는 속도와 방향**을 나타냅니다. 함수의 기울기를 구한다는 것은, 그 함수에서 **어떤 점에서의 변화율**을 계산하는 것입니다. 다시 말해, 기울기는 함수의 기울기(변화의 속도)를 알려주고, 이 정보를 통해 우리는 함수가 **어떤 방향으로 변하고 있는지** 알 수 있다.

<br>

### **경사하강법(Gradient Descent)**

**경사하강법**은 함수의 **최솟값을 찾는 최적화 알고리즘**입니다. 경사하강법에서 중요한 개념은 바로 기울기(gradient)인데, 경사하강법은 **기울기를 이용해 함수의 최솟값을 찾는 방법**이다.

1. **기울기를 활용한 방향 찾기**:
   경사하강법은 주어진 함수의 **기울기를 계산**해서 **기울기가 가장 작은 방향으로** 점차 이동한다. 즉, **기울기**를 따라 가면 함수의 **값이 감소하는 방향**으로 가게 되기 때문에, **최솟값**을 향해 점진적으로 이동할 수 있다..
2. **기울기를 이용해 업데이트하기**:
   기울기를 계산한 뒤, 그 기울기가 나타내는 **방향으로 파라미터(세타)를 업데이트**한다. 기울기가 **양수**라면 그 방향으로 가는 것이 **증가하는 방향**이고, **음수**라면 **감소하는 방향**입니다. 그래서 경사하강법에서는 기울기의 반대 방향으로 **파라미터를 업데이트**하면서 손실 함수(Loss function)가 최소가 되는 지점을 찾는다.

<br>

### **기울기와 경사하강법의 관계**

경사하강법에서 기울기는 매우 중요한 역할을 한다.
경사하강법은 손실 함수(Loss function)의 기울기(gradient)를 계산하고, 그 기울기가 **최소가 되는 점**으로 이동하는 방식이다. 기울기(gradient)는 함수의 **변화율**을 나타내므로, 이를 기반으로 함수가 **어디로 가야 최소값에 도달할 수 있는지**를 알 수 있다.

<br>

### 기울기 계산 예시 (손실 함수 예시)

예를 들어, 우리가 선형 회귀 모델을 학습시키려고 한다고 한다. **손실 함수**는 모델이 예측한 값과 실제 값 사이의 차이를 측정하는 함수이고, 이 함수의 **기울기**를 계산하여 모델 파라미터(세타)를 업데이트하는 것이 **경사하강법**이다.

- \*기울기(Gradient)는 함수에서 **각 파라미터(세타)의 변화율**을 알려주고, 경사하강법은 그 기울기의 방향으로 파라미터를 업데이트하여 손실 함수 값을 최소화하려고 한다.
- **기울기**가 **양수**라면, 파라미터를 **줄여야** 손실 함수가 줄어들고, 기울기가 **음수**라면 파라미터를 **늘려야** 손실 함수가 줄어든다는 것을 의미한다.

$$
J\left(\theta_0, \theta_1\right)=\frac{1}{2 m} \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2
$$

$h_\theta\left(x^{(i)}\right)=\theta_0+\theta_1 x^{(i)}$는 예측 값이다.

$y^{(i)}$는 실제 값이고,

$m$은 데이터의 개수이다.

<br>

## 편미분을 하는 이유

### 1. 다변수 함수에서 각 변수의 기울기가 중요한 이유

다변수 함수에서는 여러 변수들이 함께 결합되어 결과를 만들어낸다. 각 변수는 함수의 결과에 **다르게 영향을 미친다**. 이를 잘 파악하려면 각 변수에 대해 **독립적으로 기울기를 구**해야 한다..

### 예시

다변수 함수 -> $f(x, y)=x^2+y^2$

**x에 대한 기울기 (편미분)**:

- $\frac{\partial}{\partial x}\left(x^2+y^2\right)=2 x$

여기서는y가 고정되고x만 변화한다. 즉,**x값이 변화함에 따라 함수 값이 얼마나 빨리 변화하는지**를 알려준다.

**y에 대한 기울기 (편미분)**:

- $\frac{\partial}{\partial x}\left(x^2+y^2\right)=2 x$
- 이 경우는 x가 고정되고 y만 변화한다. 즉, **y값이 변화할 때 함수 값이 얼마나 변화하는지**를 알려준다.

<br>

### 2. 다변수 함수에서 기울기를 구하는 이유

### (1) **변수 간의 관계 파악**

각 변수의 변화가 함수에 미치는 영향을 개별적으로 파악할 수 있다. 예를 들어, **x**가 함수의 결과에 더 큰 영향을 미친다면 x에 대한 기울기가 더 크고, **y**의 기울기가 작다면 y는 함수 결과에 미치는 영향이 적다는 것을 알 수 있다.

### (2) **최적화 및 경사하강법**

경사하강법에서는 다변수 함수에서 **각 변수의 기울기**를 구하여 그 값을 **조정**한다. 각 변수에 대한 기울기(편미분)를 계산하고, 이 값을 이용해 **파라미터 값을 업데이트**하여 **최소값**이나 **최적값**을 찾는다. 기울기가 **큰 방향**으로 값을 변경해야 최소화가 될 가능성이 높기 때문이다.

### (3) **함수의 기울기를 1개만 구하기 어려움**

다변수 함수에서 한 변수의 기울기만 구해봐야, 다른 변수들에 의한 영향을 알 수 없다. 예를 들어, **기울기**는 함수의 **기울어진 정도**를 의미하는데, 한 변수만으로는 **다른 변수들의 영향을 고려할 수 없기 때문**이다.

### (4) **다변수 함수에서 모든 변수의 변화율을 알기 위해**

여러 변수들이 동시에 함수에 영향을 미치므로, 각 변수에 대한 기울기를 구해야 **전체 함수의 변화율**을 제대로 파악할 수 있습니다. 이는 최적화, 머신러닝 모델 훈련 등에 필수적인 과정이다.

<br>

### 3. 기울기를 여러 개 구하면 어떤 일이 발생할까?

다변수 함수에서 여러 기울기를 구하는 이유는, 각 변수들이 함수에 미치는 **영향을 독립적으로 이해**하고, 이 정보를 바탕으로 **함수의 최적화 과정**을 제대로 수행하기 위해서다.

예를 들어, 다변수 함수에서 기울기를 구할 때:

x에 대한 기울기$\frac{\partial}{\partial x}$는 **x가 함수의 출력에 얼마나 영향을 미치는지** 를 알려주고,

y에 대한 기울기$\frac{\partial}{\partial y}$는 y**가 함수의 출력에 얼마나 영향을 미치는지** 를 알려준다.

이 두 기울기를 사용하여 $x$와$y$의 값을 조정해가며 **함수 값**이 **최솟값**에 가까워지도록 한다.

<br>

## 정리

가설 함수

- 각 변수에 따라 어떤 값이 나올지 예상하는 함수. 그래서 이름도 가설함수
- 가설 함수를 쓰는 이유 → 변수를 이리저리 입력해서 최적선을 찾으려고

<br>

최적선

- 주어진 데이터에 가장 알맞은 선

<br>

손실 함수

- 가설함수는 데이터의 값을 예상하는 함수이다. 따라서 실제 값과 차이가 있다. 얼마나 차이가 있는지 손실함수로 가설함수를 계산해 점수로 평가한다.
- 즉, 가설 함수에 이리저리 막 변수 값을 대입하면서, 그때마다 손실함수를 계산한다. 그때마다 평가 점수가 다르므로, 손실함수 계산값인 평가 점수가 가장 최적인 것을 구한다.

<br>

경사하강법

- 최적선을 구할때 가설함수에 변수를 넣는다고 했다. 이때 무작위로 막넣는게 아니다. 경사하강법이라는 방법으로 변수값을 적절하게 넣어준다.
- 이름 그대로 경사가 있으면 그대로 타고 내려가 가장 아래값을 구하는것이다. 최적점을 구한다는 뜻.(점수가 낮으면 좋은거라고 생각해도됌)

<br>

편미분 하는이유

- 미분을 한다는 것은 순간기울기를 구하는 것이다. 기울기는 **변화하는 속도와 방향**을 알려준다. 변화하는 속도는 기울기가 클수록 속도가 높다는 뜻이다. 방향은 기울기가 / ← 이렇게 되어있으면 오른쪽 위로 올라가는것이다. 방향을 알려준다는 의미이다.
- 편미분은 다변항일때 미분을 하는것이다. 편미분을 통해 원하는 변수만을 남기고 싹대 쳐낸다. 그리고 해당 변수만이 y값, 즉 결과값에 영향을 얼마나 끼치는지 알수있다. 예를들어 $\frac{\partial}{\partial x}\left(x^2+y^2\right)=2 x$. 위의 예시 처럼 y는 x값에 따라 기울기 값이 변한다. 이렇게 각각 변수별로 기울기를 다 구해서 y값, 즉 결과값에 얼마나 영향을 끼치는지 각각 파악한다. 여러 변수들이 동시에 함수에 영향을 미치므로, 각 변수에 대한 기울기를 구해야 **전체 함수의 변화율**을 제대로 파악할 수 있다..

<br>

정리

경사하강법으로 변수를 구함.

- 만약 다변항이면 편미분 해주어야함.

가설함수에 구했던 변수를 대입.

손실함수로 가설함수를 계산해 점수를 매김.

무한 반복.

가장 좋은 손실함수가 나오면 그때 최적점이다.
