- [로지스틱 회귀 경사하강법](#로지스틱-회귀-경사하강법)
  - [예측값 계산(시그모이드 함수)](#예측값-계산시그모이드-함수)
  - [오차값 계산](#오차값-계산)
  - [손실함수(비용함수)](#손실함수비용함수)
  - [세타(𝜃) 업데이트 - 편미분을 활용한 경사 하강법](#세타𝜃-업데이트---편미분을-활용한-경사-하강법)
  - [손실함수 편미분](#손실함수-편미분)
  - [세타 업데이트](#세타-업데이트)
  - [정리](#정리)
  - [이해 안됐던점](#이해-안됐던점)

---

로지스틱 회귀 경사하강법 프로세스

1. 시그모이드 함수를 이용해 예측값 $h_θ​(x)$을 구한다. → $h_θ​(x)$ = `sigmoid(X$𝜃$)`
2. 예측값과 실제값의 차이(오차)를 계산한다. →`h(x) - y`
3. 오차를 기반으로 비용 함수 $J(θ)$을 최소화하는 방향으로 세타($𝜃$)를 업데이트한다.
4. 각 세타($𝜃$)에 대해 편미분한 값을 이용하여 경사 하강법을 적용한다.
   - $θ_j​:=θ_j​−α⋅\frac{1}m​∑(h_θ​(x)−y)x_j​$
5. $\theta_j:= \theta_j - \alpha ⋅ 편미분 값$ 을 통해 새로운 세타 값을 만든다.

<br>

# 로지스틱 회귀 경사하강법

선형회귀는 최적선을 구하기 위해 사용한다.

로지스틱 회귀는 **데이터를 두개로 잘 분류할 수 있는 최적의 결정 경계**를 찾는것이다.

<br>

### 예측값 계산(시그모이드 함수)

선형회귀의 최적의 선을 찾기위해서는 1차식 가정함수를 사용한다.

로직스틱 회귀는 최적의 결정 경계를 찾기위해 **시그모이드 함수**를 사용한다.

<br>

**시그모이드(Sigmoid) 함수**를 사용하여 예측값을 0~1 사이의 확률로 변환한다.

$$
h_\theta(x)=\frac{1}{1+e^{\theta_0+\theta_1x_1+\theta_2x_2....+\theta_nx_n}}
$$

![image.png](attachment:f928c54e-e672-4c82-ac49-14398b296964:image.png)

- 만약 $h_\theta(x)≥0.5$이면 1로 분류
- $h_\theta(x)<0.5$이면 0으로 분류

<br>

$h_\theta(x)$ : 예측 확률값 (0~1 사이)

θ : 학습해야 할 가중치(세타)

$x$: 입력 데이터 (특성)

<br>

### 오차값 계산

경사 하강법을 적용하려면, 예측값과 실제값의 차이를 계산해야 한다.

<br>

$Error = 예측값 - 실제값$

$\text { Error }=h_\theta(x)-y$

- $h_\theta(x)$ : 예측값
- $y$ : 실제값 (0 또는 1)

<br>

즉, 예측값과 실제값의 차이를 계산하면, **얼마나 잘못 예측했는지**를 알 수 있다.

이 값은 개별 데이터에 대한 오차값를 나타낸다.

따라서 각 개별 오차값을 이용해서 전체적인 손실을 측정해야한다.

<br>

### 손실함수(비용함수)

모든 데이터 샘플의 오차를 고려한 **평균적인 손실(loss)**을 나타냄.

로지스틱 회귀는 MSE(평균 제곱 오차) 대신 로그 손실(Log Loss, 이진 크로스 엔트로피)을 사용한다.

$$
J(θ)=-\frac{1}{m}∑(ylogh_θ(x)+(1−y)log(1−h_θ(x)))
$$

이 비용 함수의 값을 가장 최소화 하는 방향으로 경사 하강법(Gradient Descent)을 적용한다.

<br>

손실함수의 최적의 결정 경계를 위한 각 $\theta$별 최적의 값을 구해야한다.

$\theta$을 지속적으로 업데이트해서 최적의 값이 나올때까지 찾아 주어야 한다.

### 세타(𝜃) 업데이트 - 편미분을 활용한 경사 하강법

### 손실함수 편미분

세타를 업데이트 하기위해 손실함수를 각 세타(𝜃)별로 편미분하면 다음과 같다.

$$
\frac{\partial J(\theta)}{\partial \theta_j}=\frac{1}{m} \sum\left(h_\theta(x)-y\right) x_j
$$

즉, (예측값 - 실제값)의 평균을 구하고, 각 특성 $x_j$에 대해서 곱해준다.

$x_j$ → 각 특성 ex) 공부시간 or 모의고사 점수

<br>

### 세타 업데이트

경사 하강법을 적용하여 세타를 업데이트한다.

$θ_j​:=θ_j​−α⋅\frac{1}m​∑(h_θ​(x)−y)x_j​$

- $\alpha$ : 학습률 (Learning Rate), 세타 값을 조정하는 속도
- $\frac{1}m​∑(h_θ​(x)−y)x_j​$: 편미분을 이용한 기울기

<br>

즉, **이전 세타 값에서 학습률 $x$ 기울기만큼 빼면서 새로운 세타 값을 만든다.**

이 과정을 여러 번 반복하면 점점 더 정확한 세타 값에 가까워진다.

<br>

## 정리

1. 시그모이드 함수를 이용해 예측값 $h_θ​(x)$을 구한다.
2. 예측값과 실제값의 차이(오차)를 계산한다.
3. 오차를 기반으로 비용 함수 $J(θ)$을 최소화하는 방향으로 세타($𝜃$)를 업데이트한다.
4. 각 세타($𝜃$)에 대해 편미분한 값을 이용하여 경사 하강법을 적용한다.
5. $\theta_j:= \theta_j - \alpha ⋅ 편미분 값$ 을 통해 새로운 세타 값을 만든다.

<br>

즉,

**기존 세타 값에서 "오차의 평균 × 학습률"을 빼면서 최적의 세타 값으로 업데이트하는 과정이다.**

반복할수록 최적의 결정 경계(Decision Boundary)를 찾아가게 된다.

<br>

## 이해 안됐던점

$θ_j=θ_j−α⋅\frac{1}m∑(h_θ(x)−y)x_j$이 코드에서 $θ←θ−α\frac{1}m​(X^T×error)$ 으로 바뀌는 이유

`(예상치 - 실제값) * X`와 `X.T * (예상치 - 실제값)`이 결과적으로 상관없다.

우리는 지금 로지스틱 회귀에서 경사하강법을 사용하여 세타(가중치)를 업데이트하고 있다.

이때 오차 벡터 `(예상치 - 실제값)`와 속성값 행렬 X를 곱하는데, 이 곱셈의 순서가 바뀌어도 결과는 상관없다.

<br>

그 이유는 우리는 최종적으로 세타를 업데이트하는 데만 관심이 있기 때문다.

세타 업데이트는 오차와 특징 간의 관계를 반영하는 것이지, 벡터나 행렬의 순서 자체가 중요한 게 아니다.

따라서, `(예상치 - 실제값) * X`와 `X.T * (예상치 - 실제값)`는 결과적으로 동일한 세타 값을 업데이트하기 때문에 순서가 바뀌어도 문제가 없다

<br>

그래서 $X^T$를 통해 error와 행렬과 벡트의 계산이 가능하도록 만들어준것이다.

행과 열을 서로 맞추어서 곱하게 하기 위해서이다.
